{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"sp500_headlines_2008_2024.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"dyutidasmahaptra/s-and-p-500-with-financial-news-headlines-20082024\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "df = df.head(100)\n",
    "\n",
    "#print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For BERT\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cleaning text\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Plots\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', soup.get_text())\n",
    "    pattern = r\"[^a-zA-Z0-9\\s,']\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "df['TitleClean'] = df['Title'].apply(text_cleaning)\n",
    "\n",
    "# Input text\n",
    "text_list = df['TitleClean'].to_list()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Mean pooling to get sentence embeddings. See:\n",
    "    https://huggingface.co/sentence-transformers/paraphrase-distilroberta-base-v1\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) # Sum columns\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encoded_input = [tokenizer(expr, return_tensors=\"pt\") for expr in text_list]\n",
    "\n",
    "# Create word embeddings\n",
    "print('Creating word embeddings')\n",
    "sentence_embeddings = []\n",
    "for i in range(len(encoded_input)) :\n",
    "    print(str(i+1)+'/'+str(len(encoded_input)))\n",
    "    model_output = model(**encoded_input[i])\n",
    "    sentence_embeddings.append(mean_pooling(model_output, encoded_input[i]['attention_mask']).detach().numpy()[0])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word cloud\n",
    "def generate_wordcloud(text,Title):\n",
    "    all_text = \" \".join(text)\n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=400,\n",
    "                          stopwords=set(STOPWORDS), \n",
    "                          background_color='black').generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_wordcloud(df['Title'].apply(text_cleaning),'  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df[['Date', 'CP']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "print(df_unique)\n",
    "df_unique['lnCP1'] = np.log(df_unique['CP'].shift(-1))\n",
    "\n",
    "df = pd.merge(\n",
    "    df,               \n",
    "    df_unique[['Date', 'lnCP1']],        \n",
    "    on='Date',        \n",
    "    how='left'        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding data set\n",
    "\n",
    "df_new = pd.concat([df, pd.DataFrame(np.array(sentence_embeddings))], axis=1)\n",
    "\n",
    "df_new.to_csv('EmbeddedData.csv', index=False)\n",
    "\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = ['lnCP1']\n",
    "X_vars = [col for col in df_new.columns if col not in [\"Title\", \"Date\", \"CP\", 'lnCP1', \"TitleClean\"]]\n",
    "\n",
    "step_size = 1\n",
    "test_size = 0.3\n",
    "\n",
    "init_test = int(len(df_unique) * (1-test_size))\n",
    "\n",
    "print(init_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(41223)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize models\n",
    "models = []\n",
    "\n",
    "models.append((\"LinearRegression\", LinearRegression()))\n",
    "models.append((\"Ridge\", Ridge()))\n",
    "models.append((\"Lasso\", Lasso()))\n",
    "models.append((\"SVR\", SVR()))\n",
    "models.append((\"KNeighbors\", KNeighborsRegressor()))\n",
    "models.append((\"DecisionTree\", DecisionTreeRegressor()))\n",
    "models.append((\"RandomForest\", RandomForestRegressor()))\n",
    "rf2 = RandomForestRegressor(n_estimators=100, criterion='squared_error',\n",
    "                           max_depth=10, random_state=0, max_features=None)\n",
    "models.append((\"RandomForest2\", rf2))\n",
    "models.append((\"GradientBoosting\", GradientBoostingRegressor()))\n",
    "models.append((\"MLPRegressor\", MLPRegressor(solver='lbfgs', random_state=0)))\n",
    "\n",
    "a = 0\n",
    "\n",
    "df_test = []\n",
    "\n",
    "# Create dictionary with empty lists\n",
    "model_pred = {name: [] for name, _ in models}\n",
    "\n",
    "while a <= len(df_unique) - init_test :\n",
    "    # For a = 0, train starts at zero and ends at init_test-2\n",
    "    # Test starts at init_test-1 and ends in init_test-1\n",
    "    train_dates, test_dates = df_unique.iloc[a:(a + init_test-1)]['Date'].to_list(), [df_unique.iloc[(a + init_test-1)]['Date']]\n",
    "\n",
    "    train, test = df_new[df_new['Date'].isin(train_dates)], df_new[df_new['Date'].isin(test_dates)]\n",
    "\n",
    "    df_test.append(test)\n",
    "    y_train = train[y_vars]\n",
    "    X_train = train[X_vars]\n",
    "\n",
    "    y_test = test[y_vars]\n",
    "    X_test = test[X_vars]\n",
    "\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        model_pred[name] += model.predict(X_test).tolist()\n",
    "\n",
    "    a += 1\n",
    "\n",
    "df_test = pd.concat(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_test))\n",
    "print(len(pd.DataFrame(model_pred)))\n",
    "\n",
    "comparison_df = pd.concat([df_test[['Date', 'lnCP1']].reset_index(drop=True), pd.DataFrame(model_pred).reset_index(drop=True)], axis = 1)\n",
    "#comparison_df = comparison_df.groupby('Date', as_index=False).mean()\n",
    "\n",
    "# First convert array columns to regular float columns\n",
    "def extract_single_value(x):\n",
    "    if isinstance(x, (list, np.ndarray)) and len(x) == 1:\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "# Apply to all columns except Date\n",
    "for col in comparison_df.columns:\n",
    "    if col != 'Date':\n",
    "        comparison_df[col] = comparison_df[col].apply(extract_single_value)\n",
    "\n",
    "comparison_df = comparison_df.groupby('Date', as_index=False).mean()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Date is datetime\n",
    "comparison_df['Date'] = pd.to_datetime(comparison_df['Date'])\n",
    "\n",
    "# Get model columns (exclude Date and lnCP1)\n",
    "model_cols = [col for col in comparison_df.columns if col not in ['Date', 'lnCP1']]\n",
    "\n",
    "# Create individual plots for each model comparison\n",
    "for model in model_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot lnCP1 (actual values)\n",
    "    plt.plot(comparison_df['Date'], comparison_df['lnCP1'], \n",
    "             'b-', linewidth=2, label='lnCP1 (Actual)')\n",
    "    \n",
    "    # Plot model predictions\n",
    "    plt.plot(comparison_df['Date'], comparison_df[model], \n",
    "             'r--', linewidth=1.5, label=f'{model} (Predicted)')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f'lnCP1 vs {model} Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "for model in model_pred.keys() :\n",
    "    y_pred = comparison_df[model]\n",
    "    mape = mean_absolute_percentage_error(y_pred[:-1], comparison_df[\"lnCP1\"][:-1])\n",
    "    rmse = np.sqrt(mean_squared_error(y_pred[:-1], comparison_df[\"lnCP1\"][:-1]))\n",
    "    print(f\"--------------------------\\nModel: {model}\\nTest MAPE: {mape:.2f}\\nTest RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
